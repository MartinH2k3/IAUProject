{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:38:09.628065Z",
     "start_time": "2024-11-09T15:38:09.620160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# just for easier debugging\n",
    "from typing import *\n",
    "# phase 1 imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from distributed.utils_test import throws\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "# phase 2 imports\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, LassoCV, Ridge, RidgeCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold, f_classif, mutual_info_classif, RFE\n",
    "# phase 3 imports\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ],
   "id": "4b190eddd736bf5e",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:06:20.764268Z",
     "start_time": "2024-11-09T15:06:20.754494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# functions to redo phase 1\n",
    "def get_outliers(column: pd.Series):\n",
    "    lower_quartile = column.quantile(0.25)\n",
    "    upper_quartile = column.quantile(0.75)\n",
    "    iqr = upper_quartile - lower_quartile\n",
    "    return column[(column < lower_quartile - 1.5*iqr) | (column > upper_quartile + 1.5*iqr)]\n",
    "def iterative_reformat(processes_ptr: pd.DataFrame, connections_ptr: pd.DataFrame) -> pd.DataFrame:\n",
    "    connections_ptr['ts'] = pd.to_datetime(connections_ptr['ts'])\n",
    "    processes_ptr['ts'] = pd.to_datetime(processes_ptr['ts'])\n",
    "    merged = processes_ptr.merge(connections_ptr, on=['ts', 'imei', 'mwra'], how='inner')\n",
    "    merged['mwra'] = merged['mwra'].astype('int64')\n",
    "    merged.drop(columns=['ts', 'imei'], inplace=True)\n",
    "    to_drop = []\n",
    "    # handle null values and outliers\n",
    "    for column in merged.columns:\n",
    "        # if more than 5% are NaN values or more than 5% are outliers, we don't use that column\n",
    "        column_outliers = get_outliers(merged[column])\n",
    "        if ((merged[column].isna().sum()/merged.shape[0] > 0.05) or \n",
    "            (column_outliers.shape[0] / merged.shape[0] > 0.05)):\n",
    "            to_drop.append(column)\n",
    "            continue\n",
    "        # if there are some null values, we replace the data that's neutral in respect to mwra\n",
    "        if merged[column].isnull().any():\n",
    "            # we get means of the distributions for rows with present and non-present malware related activity\n",
    "            means_per_mwra = merged.groupby('mwra')[column].mean()\n",
    "            # we average those means, meaning the manufactured value won't be likely to affect predicted mwra \n",
    "            imputed_value = means_per_mwra.mean()\n",
    "            merged[column].fillna(imputed_value, inplace=True)\n",
    "        #  if there are any outliers, we replace them with the edge values. If we clipped all outliers, we would clutter way too much data together, so we clip only the most extreme ones\n",
    "        if column_outliers.shape[0]:\n",
    "            iqr = stats.iqr(merged[column])\n",
    "            lower_limit = merged[column].quantile(0.25)  - 2.5 * iqr\n",
    "            upper_limit = merged[column].quantile(0.75)  + 2.5 * iqr\n",
    "            merged[column] = merged[column].clip(lower=lower_limit, upper=upper_limit)\n",
    "    return merged.drop(columns=to_drop)\n",
    "# functions to redo phase 2"
   ],
   "id": "67d48d526a25a659",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:06:24.623600Z",
     "start_time": "2024-11-09T15:06:24.408437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# getting the data from previous phases\n",
    "connections, processes, = pd.read_csv('data/connections.csv', sep='\\t', keep_default_na=False, na_values=''), pd.read_csv('data/processes.csv', sep='\\t', keep_default_na=False, na_values=''),\n",
    "combined_table = iterative_reformat(processes, connections)\n",
    "X = combined_table.drop(columns=['mwra'])\n",
    "y = combined_table['mwra']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)"
   ],
   "id": "90741fe1f4f969b4",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Phase 3: Machine Learning",
   "id": "911acb1d8b313572"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b28ff85e1c26f39c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we are dealing with continuous data, we will be using a different version of the ID3 algorithm opposed to the one shown in the lecture.  \n",
    "Our implementation was inspired by the following implementation: https://www.geeksforgeeks.org/iterative-dichotomiser-3-id3-algorithm-from-scratch/"
   ],
   "id": "bb1dfb0188104bd9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:49:04.571536Z",
     "start_time": "2024-11-09T15:49:04.558285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_entropy(data):\n",
    "    counts = np.bincount(data)\n",
    "    probabilities = counts / len(data)\n",
    "    return entropy(probabilities, base=2) # base=2 to use log2\n",
    "\n",
    "\n",
    "class ID3classifier:\n",
    "    class Node:\n",
    "        def __init__(self, feature=None, result=None, value=None, left_child=None, right_child=None):\n",
    "            self.feature: np.array = feature # column index of criteria being tested\n",
    "            self.result: Union[0, 1] = result\n",
    "            self.split_point: int = value\n",
    "            self.left_child: Optional['ID3classifier.Node'] = left_child\n",
    "            self.right_child: Optional['ID3classifier.Node'] = right_child\n",
    "            \n",
    "    def __init__(self, max_depth=None, min_samples_split=2, threshold=0):\n",
    "        self.root: Optional['ID3classifier.Node'] = None\n",
    "        self.max_depth: int = max_depth\n",
    "        self.min_samples_split: int = min_samples_split\n",
    "        self.threshold: float = threshold\n",
    "        \n",
    "    def fit(self, data_samples: list[list[float]], target_values: list[int]):\n",
    "        self.root = self._build(data_samples, target_values)\n",
    "        \n",
    "    def _build(self, X, y, depth=0):\n",
    "        # going through halt conditions\n",
    "        if (len(set(y)) == 1 or \n",
    "                (self.max_depth is not None and depth >= self.max_depth) or\n",
    "                (len(y) < self.min_samples_split)):\n",
    "            return self.Node(result=Counter(y).most_common(1)[0][0])\n",
    "        \n",
    "        best_entropy_gain = 0\n",
    "        best_feature = None\n",
    "        best_split_point = None\n",
    "        features_length = X.shape[1]\n",
    "        current_entropy = calculate_entropy(y)\n",
    "        best_left_X, best_left_y, best_right_X, best_right_y = None, None, None, None\n",
    "        for feature in range(features_length):\n",
    "            split_point = np.median(X[:, feature])\n",
    "            left_indices = X[:, feature] < split_point\n",
    "            right_indices = X[:, feature] >= split_point\n",
    "            left_X, left_y = X[left_indices], y[left_indices]\n",
    "            right_X, right_y = X[right_indices], y[right_indices]\n",
    "            left_entropy = calculate_entropy(left_y)\n",
    "            right_entropy = calculate_entropy(right_y)\n",
    "            left_p = len(left_y) / len(y)\n",
    "            right_p = 1 - left_p\n",
    "            entropy_gain = current_entropy - (left_p * left_entropy + right_p * right_entropy)\n",
    "            \n",
    "            if entropy_gain > best_entropy_gain:\n",
    "                best_entropy_gain = entropy_gain\n",
    "                best_feature = feature\n",
    "                best_split_point = split_point\n",
    "                best_left_X, best_left_y, best_right_X, best_right_y = left_X, left_y, right_X, right_y\n",
    "                \n",
    "        if best_entropy_gain > self.threshold:\n",
    "            left_child = self._build(best_left_X, best_left_y, depth + 1)\n",
    "            right_child = self._build(best_right_X, best_right_y, depth + 1)\n",
    "            return self.Node(feature=best_feature, value=best_split_point ,left_child=left_child, right_child=right_child)\n",
    "        \n",
    "        # if no split found, return the most common label by default\n",
    "        return self.Node(result=Counter(y).most_common(1)[0][0])\n",
    "    \n",
    "    def predict(self, item):\n",
    "        if self.root is None:\n",
    "            raise ValueError('The model has not been trained yet.')\n",
    "            \n",
    "        # the depth has to be at least 2, including root\n",
    "        if self.root.left_child is None and self.root.right_child is None:\n",
    "            return None\n",
    "        return self._predict(self.root, item)\n",
    "        \n",
    "    def _predict(self, node, item):\n",
    "        if node.result is not None:\n",
    "            return node.result\n",
    "        if item[node.feature] < node.split_point:\n",
    "            return self._predict(node.left_child, item)\n",
    "        return self._predict(node.right_child, item)\n",
    "        "
   ],
   "id": "1c380d26f939b8fd",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:50:22.559800Z",
     "start_time": "2024-11-09T15:49:54.026374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classifier = ID3classifier()\n",
    "classifier.fit(X_train.values, y_train.values)"
   ],
   "id": "b010b307dcb864c5",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:51:30.522480Z",
     "start_time": "2024-11-09T15:51:30.471245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred = np.array([classifier.predict(item) for item in X_test.values])\n",
    "y_pred_train = np.array([classifier.predict(item) for item in X_train.values])"
   ],
   "id": "d874bc1d676160e1",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:51:35.742073Z",
     "start_time": "2024-11-09T15:51:35.730278Z"
    }
   },
   "cell_type": "code",
   "source": "accuracy_score(y_test, y_pred), accuracy_score(y_train, y_pred_train)",
   "id": "25829b349e0fb8a1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8338192419825073, 1.0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:49:24.818169Z",
     "start_time": "2024-11-09T15:49:24.808510Z"
    }
   },
   "cell_type": "code",
   "source": "precision_score(y_test, y_pred, average=None)",
   "id": "7f01bd8e37123a04",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78366638, 0.86363636])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c2e848e0758730b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import multiprocessing as mp",
   "id": "e981a0ed58cd2805"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
