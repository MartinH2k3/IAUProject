{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b190eddd736bf5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:38:09.628065Z",
     "start_time": "2024-11-09T15:38:09.620160Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d48d526a25a659",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:06:20.764268Z",
     "start_time": "2024-11-09T15:06:20.754494Z"
    }
   },
   "outputs": [],
   "source": [
    "# functions to redo phase 1\n",
    "def get_outliers(column: pd.Series):\n",
    "    lower_quartile = column.quantile(0.25)\n",
    "    upper_quartile = column.quantile(0.75)\n",
    "    iqr = upper_quartile - lower_quartile\n",
    "    return column[(column < lower_quartile - 1.5*iqr) | (column > upper_quartile + 1.5*iqr)]\n",
    "def iterative_reformat(processes_ptr: pd.DataFrame, connections_ptr: pd.DataFrame) -> pd.DataFrame:\n",
    "    connections_ptr['ts'] = pd.to_datetime(connections_ptr['ts'])\n",
    "    processes_ptr['ts'] = pd.to_datetime(processes_ptr['ts'])\n",
    "    merged = processes_ptr.merge(connections_ptr, on=['ts', 'imei', 'mwra'], how='inner')\n",
    "    merged['mwra'] = merged['mwra'].astype('int64')\n",
    "    merged.drop(columns=['ts', 'imei'], inplace=True)\n",
    "    to_drop = []\n",
    "    # handle null values and outliers\n",
    "    for column in merged.columns:\n",
    "        # if more than 5% are NaN values or more than 5% are outliers, we don't use that column\n",
    "        column_outliers = get_outliers(merged[column])\n",
    "        if ((merged[column].isna().sum()/merged.shape[0] > 0.05) or \n",
    "            (column_outliers.shape[0] / merged.shape[0] > 0.05)):\n",
    "            to_drop.append(column)\n",
    "            continue\n",
    "        # if there are some null values, we replace the data that's neutral in respect to mwra\n",
    "        if merged[column].isnull().any():\n",
    "            # we get means of the distributions for rows with present and non-present malware related activity\n",
    "            means_per_mwra = merged.groupby('mwra')[column].mean()\n",
    "            # we average those means, meaning the manufactured value won't be likely to affect predicted mwra \n",
    "            imputed_value = means_per_mwra.mean()\n",
    "            merged[column].fillna(imputed_value, inplace=True)\n",
    "        #  if there are any outliers, we replace them with the edge values. If we clipped all outliers, we would clutter way too much data together, so we clip only the most extreme ones\n",
    "        if column_outliers.shape[0]:\n",
    "            iqr = stats.iqr(merged[column])\n",
    "            lower_limit = merged[column].quantile(0.25)  - 2.5 * iqr\n",
    "            upper_limit = merged[column].quantile(0.75)  + 2.5 * iqr\n",
    "            merged[column] = merged[column].clip(lower=lower_limit, upper=upper_limit)\n",
    "    return merged.drop(columns=to_drop)\n",
    "# functions to redo phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90741fe1f4f969b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:06:24.623600Z",
     "start_time": "2024-11-09T15:06:24.408437Z"
    }
   },
   "outputs": [],
   "source": [
    "# getting the data from previous phases\n",
    "connections, processes, = pd.read_csv('data/connections.csv', sep='\\t', keep_default_na=False, na_values=''), pd.read_csv('data/processes.csv', sep='\\t', keep_default_na=False, na_values=''),\n",
    "combined_table = iterative_reformat(processes, connections)\n",
    "\n",
    "# JANKA added\n",
    "columns_to_divide = combined_table.columns.difference(['mwra'])\n",
    "combined_table[columns_to_divide] = combined_table[columns_to_divide].apply(lambda x: x / 2)\n",
    "combined_table = combined_table.astype('int64') \n",
    "\n",
    "X = combined_table.drop(columns=['mwra'])\n",
    "y = combined_table['mwra']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911acb1d8b313572",
   "metadata": {},
   "source": [
    "# Phase 3: Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1dfb0188104bd9",
   "metadata": {},
   "source": [
    "As we are dealing with continuous data, we will be using a different version of the ID3 algorithm opposed to the one shown in the lecture.  \n",
    "Our implementation was inspired by the following implementation: https://www.geeksforgeeks.org/iterative-dichotomiser-3-id3-algorithm-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c380d26f939b8fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:49:04.571536Z",
     "start_time": "2024-11-09T15:49:04.558285Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_entropy(data):\n",
    "    counts = np.bincount(data)\n",
    "    probabilities = counts / len(data)\n",
    "    return entropy(probabilities, base=2) # base=2 to use log2\n",
    "\n",
    "\n",
    "class ID3classifier:\n",
    "    class Node:\n",
    "        def __init__(self, feature=None, result=None, value=None, left_child=None, right_child=None):\n",
    "            self.feature: np.array = feature # column index of criteria being tested\n",
    "            self.result: Union[0, 1] = result\n",
    "            self.split_point: int = value\n",
    "            self.left_child: Optional['ID3classifier.Node'] = left_child\n",
    "            self.right_child: Optional['ID3classifier.Node'] = right_child\n",
    "            \n",
    "    def __init__(self, max_depth=None, min_samples_split=2, threshold=0):\n",
    "        self.root: Optional['ID3classifier.Node'] = None\n",
    "        self.max_depth: int = max_depth\n",
    "        self.min_samples_split: int = min_samples_split\n",
    "        self.threshold: float = threshold\n",
    "        \n",
    "    def fit(self, data_samples: list[list[float]], target_values: list[int]):\n",
    "        self.root = self._build(data_samples, target_values)\n",
    "        \n",
    "    def _build(self, X, y, depth=0):\n",
    "        # going through halt conditions\n",
    "        if (len(set(y)) == 1): # JANKA changed condition\n",
    "            return self.Node(result=Counter(y).most_common(1)[0][0])\n",
    "        \n",
    "        best_entropy_gain = 0\n",
    "        best_feature = None\n",
    "        best_split_point = None\n",
    "        features_length = X.shape[1]\n",
    "        current_entropy = calculate_entropy(y)\n",
    "        best_left_X, best_left_y, best_right_X, best_right_y = None, None, None, None\n",
    "        for feature in range(features_length):\n",
    "            #JANKA changed - we are looking for the best split point (= split with highest entropy gain)\n",
    "            feature_values = set(X[:, feature])\n",
    "            for split_point in feature_values:\n",
    "                left_indices = X[:, feature] < split_point\n",
    "                right_indices = X[:, feature] >= split_point\n",
    "                left_X, left_y = X[left_indices], y[left_indices]\n",
    "                right_X, right_y = X[right_indices], y[right_indices]\n",
    "                left_entropy = calculate_entropy(left_y)\n",
    "                right_entropy = calculate_entropy(right_y)\n",
    "                left_p = len(left_y) / len(y)\n",
    "                right_p = 1 - left_p\n",
    "                entropy_gain = current_entropy - (left_p * left_entropy + right_p * right_entropy)\n",
    "                if entropy_gain > best_entropy_gain:\n",
    "                    best_entropy_gain = entropy_gain\n",
    "                    best_feature = feature\n",
    "                    best_split_point = split_point\n",
    "                    best_left_X, best_left_y, best_right_X, best_right_y = left_X, left_y, right_X, right_y\n",
    "                \n",
    "        if best_entropy_gain > self.threshold:\n",
    "            left_child = self._build(best_left_X, best_left_y, depth + 1)\n",
    "            right_child = self._build(best_right_X, best_right_y, depth + 1)\n",
    "            return self.Node(feature=best_feature, value=best_split_point ,left_child=left_child, right_child=right_child)\n",
    "        \n",
    "        # if no split found, return the most common label by default\n",
    "        return self.Node(result=Counter(y).most_common(1)[0][0])\n",
    "    \n",
    "    def predict(self, item):\n",
    "        if self.root is None:\n",
    "            raise ValueError('The model has not been trained yet.')\n",
    "            \n",
    "        # the depth has to be at least 2, including root\n",
    "        if self.root.left_child is None and self.root.right_child is None:\n",
    "            return None\n",
    "        return self._predict(self.root, item)\n",
    "        \n",
    "    def _predict(self, node, item):\n",
    "        if node.result is not None:\n",
    "            return node.result\n",
    "        if item[node.feature] < node.split_point:\n",
    "            return self._predict(node.left_child, item)\n",
    "        return self._predict(node.right_child, item)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010b307dcb864c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:50:22.559800Z",
     "start_time": "2024-11-09T15:49:54.026374Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = ID3classifier()\n",
    "classifier.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874bc1d676160e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:51:30.522480Z",
     "start_time": "2024-11-09T15:51:30.471245Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test = np.array([classifier.predict(item) for item in X_test.values])\n",
    "y_pred_train = np.array([classifier.predict(item) for item in X_train.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25829b349e0fb8a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:51:35.742073Z",
     "start_time": "2024-11-09T15:51:35.730278Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_test), accuracy_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f01bd8e37123a04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:49:24.818169Z",
     "start_time": "2024-11-09T15:49:24.808510Z"
    }
   },
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred_test, average=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
